{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "068e27ac",
   "metadata": {},
   "source": [
    "### Importing and Loading the Data from pickle files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a38991d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#General Imports\n",
    "import os \n",
    "import sys\n",
    "from dotenv import load_dotenv #importing env file\n",
    "import pandas as pd\n",
    "import random\n",
    "import pickle\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0fdbb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from src.advanced_tester import Tester"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a5bd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ML Imports\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c4ebe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NLP\n",
    "import gensim\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd43d9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = '/Users/vishesh/projects/Pricer/data/balanced/train.pkl'\n",
    "test_path = '/Users/vishesh/projects/Pricer/data/balanced/test.pkl'\n",
    "with open(train_path,'rb') as f:\n",
    "    train = pickle.load(f)\n",
    "with open(test_path,'rb') as f:\n",
    "    test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40eaabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train['title'].iloc[2344])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da2442d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(train['prompt'].iloc[2344])\n",
    "print(test['test_prompt'].iloc[2344])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e999c06e",
   "metadata": {},
   "source": [
    "### TESTER CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca54079",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d24181",
   "metadata": {},
   "source": [
    "### Basic testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef6919b",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "\n",
    "# 2. Define any predictor function\n",
    "def random_pricer(item):\n",
    "    return random.randrange(1, 1000)\n",
    "\n",
    "# 3. Test any function\n",
    "Tester.test(random_pricer,test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8221aee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#guessing average \n",
    "average = train['price'].mean()\n",
    "\n",
    "def average_pricer(item):\n",
    "    return average\n",
    "Tester.test(average_pricer,test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01743c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af836f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Training is missing the raw details so , \n",
    "I processed the data again this time including the raw details \n",
    "stored in data Raw\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0be7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_details_path = '/Users/vishesh/projects/Pricer/data/raw/train_details.pkl'\n",
    "test_details_path = '/Users/vishesh/projects/Pricer/data/raw/test_details.pkl'\n",
    "with open(train_details_path,'rb') as f:\n",
    "    train_details = pickle.load(f)\n",
    "with open(test_details_path , 'rb') as f :\n",
    "    test_details = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57af7504",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_details[234].details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8146310a",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac57336",
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting string to dictionary for extracting data/features easily using json \n",
    "#in a new features field populated with json from details dict\n",
    "import json\n",
    "for detail in train_details:\n",
    "    detail.features = json.loads(detail.details)\n",
    "for detail in test_details:\n",
    "    detail.features = json.loads(detail.details)\n",
    "\n",
    "train_details[0].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d0115e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the most common features \n",
    "from collections import Counter\n",
    "feature_count = Counter()\n",
    "for item in train_details:\n",
    "    for f in item.features.keys():\n",
    "        feature_count[f]+=1\n",
    "feature_count.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a75226b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#I believe Item Weight Manufacturer brand and Sellers Rank can be useful \n",
    "#diving into each categories for better knowledge "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c8406a",
   "metadata": {},
   "outputs": [],
   "source": [
    "brands = Counter() \n",
    "for item in train_details:\n",
    "    brand = item.features.get('Brand') \n",
    "    if brand :\n",
    "        brands[brand] +=1\n",
    "brands.most_common(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70dfc0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#looking the brands it seems like top brands are related to electronics \n",
    "TOP_ELECTRONICS_BRANDS = ['hp','dell','lenovo','samsung','asus','sony','canon','apple','intel']\n",
    "\n",
    "PREMIUM_ELECTRONICS_BRANDS = ['apple', 'sony', 'canon', 'samsung']\n",
    "\n",
    "TOP_AUTO_PARTS_BRANDS = ['power stop', 'detroit axle', 'dorman', 'buyautoparts!', 'acdelco', \n",
    "                         'evan fischer', 'callahan brake parts', 'r1 concepts', 'rareelectrical',\n",
    "                         'garage-pro', 'spectra premium', 'auto dynasty', 'cardone', 'aps', \n",
    "                         'gm', 'walker', 'ebc brakes', 'akkon', 'spec-d tuning', 'tyc', 'a-premium']\n",
    "\n",
    "TOP_AUTO_ACCESSORIES_BRANDS = ['curt', 'coverking', 'weathertech', 'covercraft', 'k&n']\n",
    "\n",
    "def is_top_electronics_brand(item):\n",
    "    brand = item.features.get(\"Brand\")\n",
    "    return brand and brand.lower() in TOP_ELECTRONICS_BRANDS\n",
    "\n",
    "def is_premium_electronics_brand(item):\n",
    "    brand = item.features.get(\"Brand\")\n",
    "    return brand and brand.lower() in PREMIUM_ELECTRONICS_BRANDS\n",
    "\n",
    "def is_top_auto_parts_brand(item):\n",
    "    brand = item.features.get(\"Brand\")\n",
    "    return brand and brand.lower() in TOP_AUTO_PARTS_BRANDS\n",
    "\n",
    "def is_top_auto_accessories_brand(item):\n",
    "    brand = item.features.get(\"Brand\")\n",
    "    return brand and brand.lower() in TOP_AUTO_ACCESSORIES_BRANDS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f33277",
   "metadata": {},
   "outputs": [],
   "source": [
    "#focusing on weight of item now \n",
    "\n",
    "\n",
    "#I see the unit of weight is not constant ( I want to stick to the grams convention)\n",
    "def get_weight(item):\n",
    "    weight_str = item.features.get('Item Weight')\n",
    "    if weight_str:\n",
    "        parts = weight_str.split(' ')\n",
    "        amount = float(parts[0])\n",
    "        unit = parts[1].lower()\n",
    "        \n",
    "        if unit == \"pounds\":\n",
    "            return amount * 453.592  #pounds to g\n",
    "        elif unit == \"ounces\":\n",
    "            return amount * 28.3495  #ounces to g\n",
    "        elif unit == \"grams\":\n",
    "            return amount  # in g\n",
    "        elif unit == \"milligrams\":\n",
    "            return amount / 1000  #mg to g\n",
    "        elif unit == \"kilograms\":\n",
    "            return amount * 1000  #kg to g\n",
    "        elif unit == \"hundredths\" and parts[2].lower() == \"pounds\":\n",
    "            return (amount / 100) * 453.592  #hundredths of pounds to g\n",
    "        else:\n",
    "            print(weight_str)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90da5d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#storing weights\n",
    "weights = [get_weight(item) for item in train_details]\n",
    "weights = [w for w in weights if w] #removing duplicates or empty "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e657e728",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_weight = sum(weights)/len(weights)\n",
    "print(f\"{average_weight:,.2f}\")\n",
    "\n",
    "#filling the empty weights with average weight \n",
    "def get_weight_with_default(item):\n",
    "    weight = get_weight(item)\n",
    "    return weight or average_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9a0ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#focusing on rank \n",
    "# single product has multiple ranks(in different product categories)\n",
    "# so taking average of all of them \n",
    "def get_rank(item):\n",
    "    rank_dict = item.features.get(\"Best Sellers Rank\")\n",
    "    if rank_dict:\n",
    "        ranks = rank_dict.values()\n",
    "        return sum(ranks)/len(ranks)\n",
    "    return None \n",
    "\n",
    "ranks=[get_rank(item) for item in train_details]\n",
    "ranks = [r for r in ranks if r]\n",
    "average_rank = sum(ranks)/len(ranks)\n",
    "print(f\"{average_rank:,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cd9bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filling empty with average rank\n",
    "def get_rank_with_default(item):\n",
    "    rank = get_rank(item)\n",
    "    return rank or average_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb30813",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_length(item):\n",
    "    return len(item.test_prompt())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e15733f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting a features dictionary \n",
    "def get_features(item):\n",
    "    return {\n",
    "        'weight':get_weight_with_default(item) ,\n",
    "        'rank':get_rank_with_default(item) ,\n",
    "        'is_top_auto_parts_brand' : 1 if is_top_auto_parts_brand(item) else 0 ,\n",
    "        'is_premium_electronics_brand': 1 if is_premium_electronics_brand(item) else 0 ,\n",
    "        'is_top_electronics_brand': 1 if is_top_electronics_brand(item) else 0 ,\n",
    "        'is_top_auto_accessories_brand': 1 if is_top_auto_accessories_brand(item) else 0 ,\n",
    "        'text_length': get_text_length(item) ,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820271f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_features(train_details[35])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d4479b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting to df \n",
    "def list_to_df(items):\n",
    "    features = [get_features(item) for item in items]\n",
    "    df = pd.DataFrame(features)\n",
    "    df['price'] = [item.price for item in items]\n",
    "    return df\n",
    "\n",
    "train_details_df = list_to_df(train_details)\n",
    "test_details_df = list_to_df(test_details)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac17b819",
   "metadata": {},
   "source": [
    "### Baseline Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d075078f",
   "metadata": {},
   "source": [
    "#### Need to have another tester class which tests on the list of the data I have provided "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d23e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tester_baseline:\n",
    "\n",
    "    def __init__(self, predictor, title=None, data=test_details, size=250):\n",
    "        self.predictor = predictor\n",
    "        self.data = data\n",
    "        self.title = title or predictor.__name__.replace(\"_\", \" \").title()\n",
    "        self.size = size\n",
    "        self.guesses = []\n",
    "        self.truths = []\n",
    "        self.errors = []\n",
    "        self.sles = []\n",
    "        self.colors = []\n",
    "\n",
    "    def color_for(self, error, truth):\n",
    "        if error<40 or error/truth < 0.2:\n",
    "            return \"green\"\n",
    "        elif error<80 or error/truth < 0.4:\n",
    "            return \"orange\"\n",
    "        else:\n",
    "            return \"red\"\n",
    "    \n",
    "    def run_datapoint(self, i):\n",
    "        datapoint = self.data[i]\n",
    "        guess = self.predictor(datapoint)\n",
    "        truth = datapoint.price\n",
    "        error = abs(guess - truth)\n",
    "        log_error = math.log(truth+1) - math.log(guess+1)\n",
    "        sle = log_error ** 2\n",
    "        color = self.color_for(error, truth)\n",
    "        title = datapoint.title if len(datapoint.title) <= 40 else datapoint.title[:40]+\"...\"\n",
    "        self.guesses.append(guess)\n",
    "        self.truths.append(truth)\n",
    "        self.errors.append(error)\n",
    "        self.sles.append(sle)\n",
    "        self.colors.append(color)\n",
    "        print(f\"{COLOR_MAP[color]}{i+1}: Guess: ${guess:,.2f} Truth: ${truth:,.2f} Error: ${error:,.2f} SLE: {sle:,.2f} Item: {title}{RESET}\")\n",
    "\n",
    "    def chart(self, title):\n",
    "        max_error = max(self.errors)\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        max_val = max(max(self.truths), max(self.guesses))\n",
    "        plt.plot([0, max_val], [0, max_val], color='deepskyblue', lw=2, alpha=0.6)\n",
    "        plt.scatter(self.truths, self.guesses, s=3, c=self.colors)\n",
    "        plt.xlabel('Ground Truth')\n",
    "        plt.ylabel('Model Estimate')\n",
    "        plt.xlim(0, max_val)\n",
    "        plt.ylim(0, max_val)\n",
    "        plt.title(title)\n",
    "        plt.show()\n",
    "\n",
    "    def report(self):\n",
    "        average_error = sum(self.errors) / self.size\n",
    "        rmsle = math.sqrt(sum(self.sles) / self.size)\n",
    "        hits = sum(1 for color in self.colors if color==\"green\")\n",
    "        title = f\"{self.title} Error=${average_error:,.2f} RMSLE={rmsle:,.2f} Hits={hits/self.size*100:.1f}%\"\n",
    "        self.chart(title)\n",
    "\n",
    "    def run(self):\n",
    "        self.error = 0\n",
    "        for i in range(self.size):\n",
    "            self.run_datapoint(i)\n",
    "        self.report()\n",
    "\n",
    "    @classmethod\n",
    "    def test(cls, function):\n",
    "        cls(function).run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb8e72d",
   "metadata": {},
   "source": [
    "#### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35787eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "train_details_df.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67730737",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = ['weight', 'rank', 'is_top_auto_parts_brand',\n",
    "       'is_premium_electronics_brand', 'is_top_electronics_brand',\n",
    "       'is_top_auto_accessories_brand', 'text_length']\n",
    "\n",
    "X_train = train_details_df[feature_columns]\n",
    "Y_train = train_details_df['price']\n",
    "x_test =test_details_df[feature_columns]\n",
    "y_test =test_details_df['price']\n",
    "\n",
    "model= LinearRegression()\n",
    "model.fit(X_train , Y_train)\n",
    "\n",
    "for feature , coef in zip(feature_columns , model.coef_):\n",
    "    print(f\"{feature}:{coef}\")\n",
    "print(f\"Intercept :{model.intercept_}\")\n",
    "\n",
    "y_pred = model.predict(x_test)\n",
    "mse = mean_squared_error(y_test , y_pred)\n",
    "r2=r2_score(y_test , y_pred)\n",
    "\n",
    "print(f\"RMSE :{(mse)**0.5}\")\n",
    "print(f\"R-squared Score :{r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f96efd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression_pricer(item):\n",
    "    features = get_features(item)\n",
    "    features_df = pd.DataFrame([features])\n",
    "    return model.predict(features_df)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69143019",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tester_baseline.test(linear_regression_pricer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d1ea09",
   "metadata": {},
   "source": [
    "#### BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476ee338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Data Preparation\n",
    "documents = train['test_prompt'].to_list()\n",
    "# test prompt so the model doesn't see the price\n",
    "np.random.seed(42)\n",
    "prices = train['price'].astype(float).to_numpy()\n",
    "# using np array maybe much better for setting random seeds \n",
    "# also LR converts it into array so better for efficiency\n",
    "\n",
    "# 2. Processing and Training\n",
    "vectorizer = CountVectorizer(max_features=1000, stop_words='english')\n",
    "X = vectorizer.fit_transform(documents)\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X, prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2580f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bow_lr_pricer(item):\n",
    "    x = vectorizer.transform([item.test_prompt])\n",
    "    return max(regressor.predict(x)[0],0)  #ensuring no negetive price is returned , also regressor.predict() return a numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02beef80",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tester.test(bow_lr_pricer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f93b67",
   "metadata": {},
   "source": [
    "#### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a720a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "processed_docs = [simple_preprocess(doc) for doc in documents]\n",
    "\n",
    "#training \n",
    "w2v_mode = Word2Vec(sentences=processed_docs, \n",
    "                        vector_size=400 , \n",
    "                        window=5,\n",
    "                        min_count=2, \n",
    "                        workers=8,\n",
    "                        epochs = 2 ,\n",
    "                    )\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedca748",
   "metadata": {},
   "outputs": [],
   "source": [
    "#taking average for each description of the product (not the best way-good for trial)\n",
    "def document_vector(doc):\n",
    "    doc_words = simple_preprocess(doc)\n",
    "    word_vectors = [w2v_mode.wv[word] for word in doc_words if word in w2v_mode.wv]\n",
    "    return np.mean(word_vectors , axis = 0) if word_vectors else np.zeros(w2v_mode.vector_size)\n",
    "\n",
    "X_w2v = np.array([document_vector(doc) for doc in documents])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880daa3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_lr_regressor = LinearRegression()\n",
    "w2v_lr_regressor.fit(X_w2v,prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da455130",
   "metadata": {},
   "outputs": [],
   "source": [
    "def w2v_lr_pricer(item):\n",
    "    doc = item.test_prompt \n",
    "    doc_vector = document_vector(doc)\n",
    "    return max(0,w2v_lr_regressor.predict([doc_vector])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a31b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tester.test(w2v_lr_pricer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb7514d",
   "metadata": {},
   "source": [
    "#### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578f4824",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45547e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "svr_regressor = LinearSVR()\n",
    "svr_regressor.fit(X_w2v, prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eff78a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def svr_pricer(item):\n",
    "    np.random.seed(42)\n",
    "    doc = item.test_prompt\n",
    "    doc_vector = document_vector(doc)\n",
    "    return max(0,float(svr_regressor.predict([doc_vector])[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a38055",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tester.test(svr_pricer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979b9f39",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797b3bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552a8c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42 , n_jobs=8)\n",
    "rf_model.fit(X_w2v , prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19370bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest_pricer(item):\n",
    "    doc = item.test_prompt\n",
    "    doc_vector = document_vector(doc)\n",
    "    return max(0,rf_model.predict([doc_vector])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd488302",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tester.test(random_forest_pricer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf377622",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
