{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "068e27ac",
   "metadata": {},
   "source": [
    "### Importing and Loading the Data from pickle files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a38991d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#General Imports\n",
    "import os \n",
    "import sys\n",
    "from dotenv import load_dotenv #importing env file\n",
    "import pandas as pd\n",
    "import random\n",
    "import pickle\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0fdbb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from src.advanced_tester import Tester"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a5bd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ML Imports\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c4ebe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NLP\n",
    "import gensim\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd43d9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = '/Users/vishesh/projects/Pricer/data/balanced/train.pkl'\n",
    "test_path = '/Users/vishesh/projects/Pricer/data/balanced/test.pkl'\n",
    "with open(train_path,'rb') as f:\n",
    "    train = pickle.load(f)\n",
    "with open(test_path,'rb') as f:\n",
    "    test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bf06e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>price</th>\n",
       "      <th>category</th>\n",
       "      <th>test_prompt</th>\n",
       "      <th>prompt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319995</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319996</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319997</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319998</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319999</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>320000 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       title  price category test_prompt prompt\n",
       "0        NaN    1.0      NaN         NaN    NaN\n",
       "1        NaN    1.0      NaN         NaN    NaN\n",
       "2        NaN    1.0      NaN         NaN    NaN\n",
       "3        NaN    1.0      NaN         NaN    NaN\n",
       "4        NaN    1.0      NaN         NaN    NaN\n",
       "...      ...    ...      ...         ...    ...\n",
       "319995   NaN    1.0      NaN         NaN    NaN\n",
       "319996   NaN    1.0      NaN         NaN    NaN\n",
       "319997   NaN    1.0      NaN         NaN    NaN\n",
       "319998   NaN    1.0      NaN         NaN    NaN\n",
       "319999   NaN    1.0      NaN         NaN    NaN\n",
       "\n",
       "[320000 rows x 5 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48587321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8000 entries, 0 to 7999\n",
      "Data columns (total 5 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   title        8000 non-null   object \n",
      " 1   price        8000 non-null   float64\n",
      " 2   category     8000 non-null   object \n",
      " 3   test_prompt  8000 non-null   object \n",
      " 4   prompt       8000 non-null   object \n",
      "dtypes: float64(1), object(4)\n",
      "memory usage: 312.6+ KB\n"
     ]
    }
   ],
   "source": [
    "test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40eaabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train['title'].iloc[2344])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da2442d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(train['prompt'].iloc[2344])\n",
    "print(test['test_prompt'].iloc[2344])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e999c06e",
   "metadata": {},
   "source": [
    "### TESTER CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca54079",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d24181",
   "metadata": {},
   "source": [
    "### Basic testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef6919b",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "\n",
    "# 2. Define any predictor function\n",
    "def random_pricer(item):\n",
    "    return random.randrange(1, 1000)\n",
    "\n",
    "# 3. Test any function\n",
    "Tester.test(random_pricer,test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8221aee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#guessing average \n",
    "average = train['price'].mean()\n",
    "\n",
    "def average_pricer(item):\n",
    "    return average\n",
    "Tester.test(average_pricer,test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01743c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af836f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Training is missing the raw details so , \n",
    "I processed the data again this time including the raw details \n",
    "stored in data Raw\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0be7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_details_path = '/Users/vishesh/projects/Pricer/data/raw/train_details.pkl'\n",
    "test_details_path = '/Users/vishesh/projects/Pricer/data/raw/test_details.pkl'\n",
    "with open(train_details_path,'rb') as f:\n",
    "    train_details = pickle.load(f)\n",
    "with open(test_details_path , 'rb') as f :\n",
    "    test_details = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57af7504",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_details[234].details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8146310a",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac57336",
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting string to dictionary for extracting data/features easily using json \n",
    "#in a new features field populated with json from details dict\n",
    "import json\n",
    "for detail in train_details:\n",
    "    detail.features = json.loads(detail.details)\n",
    "for detail in test_details:\n",
    "    detail.features = json.loads(detail.details)\n",
    "\n",
    "train_details[0].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d0115e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the most common features \n",
    "from collections import Counter\n",
    "feature_count = Counter()\n",
    "for item in train_details:\n",
    "    for f in item.features.keys():\n",
    "        feature_count[f]+=1\n",
    "feature_count.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a75226b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#I believe Item Weight Manufacturer brand and Sellers Rank can be useful \n",
    "#diving into each categories for better knowledge "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c8406a",
   "metadata": {},
   "outputs": [],
   "source": [
    "brands = Counter() \n",
    "for item in train_details:\n",
    "    brand = item.features.get('Brand') \n",
    "    if brand :\n",
    "        brands[brand] +=1\n",
    "brands.most_common(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70dfc0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#looking the brands it seems like top brands are related to electronics \n",
    "TOP_ELECTRONICS_BRANDS = ['hp','dell','lenovo','samsung','asus','sony','canon','apple','intel']\n",
    "\n",
    "PREMIUM_ELECTRONICS_BRANDS = ['apple', 'sony', 'canon', 'samsung']\n",
    "\n",
    "TOP_AUTO_PARTS_BRANDS = ['power stop', 'detroit axle', 'dorman', 'buyautoparts!', 'acdelco', \n",
    "                         'evan fischer', 'callahan brake parts', 'r1 concepts', 'rareelectrical',\n",
    "                         'garage-pro', 'spectra premium', 'auto dynasty', 'cardone', 'aps', \n",
    "                         'gm', 'walker', 'ebc brakes', 'akkon', 'spec-d tuning', 'tyc', 'a-premium']\n",
    "\n",
    "TOP_AUTO_ACCESSORIES_BRANDS = ['curt', 'coverking', 'weathertech', 'covercraft', 'k&n']\n",
    "\n",
    "def is_top_electronics_brand(item):\n",
    "    brand = item.features.get(\"Brand\")\n",
    "    return brand and brand.lower() in TOP_ELECTRONICS_BRANDS\n",
    "\n",
    "def is_premium_electronics_brand(item):\n",
    "    brand = item.features.get(\"Brand\")\n",
    "    return brand and brand.lower() in PREMIUM_ELECTRONICS_BRANDS\n",
    "\n",
    "def is_top_auto_parts_brand(item):\n",
    "    brand = item.features.get(\"Brand\")\n",
    "    return brand and brand.lower() in TOP_AUTO_PARTS_BRANDS\n",
    "\n",
    "def is_top_auto_accessories_brand(item):\n",
    "    brand = item.features.get(\"Brand\")\n",
    "    return brand and brand.lower() in TOP_AUTO_ACCESSORIES_BRANDS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f33277",
   "metadata": {},
   "outputs": [],
   "source": [
    "#focusing on weight of item now \n",
    "\n",
    "\n",
    "#I see the unit of weight is not constant ( I want to stick to the grams convention)\n",
    "def get_weight(item):\n",
    "    weight_str = item.features.get('Item Weight')\n",
    "    if weight_str:\n",
    "        parts = weight_str.split(' ')\n",
    "        amount = float(parts[0])\n",
    "        unit = parts[1].lower()\n",
    "        \n",
    "        if unit == \"pounds\":\n",
    "            return amount * 453.592  #pounds to g\n",
    "        elif unit == \"ounces\":\n",
    "            return amount * 28.3495  #ounces to g\n",
    "        elif unit == \"grams\":\n",
    "            return amount  # in g\n",
    "        elif unit == \"milligrams\":\n",
    "            return amount / 1000  #mg to g\n",
    "        elif unit == \"kilograms\":\n",
    "            return amount * 1000  #kg to g\n",
    "        elif unit == \"hundredths\" and parts[2].lower() == \"pounds\":\n",
    "            return (amount / 100) * 453.592  #hundredths of pounds to g\n",
    "        else:\n",
    "            print(weight_str)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90da5d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#storing weights\n",
    "weights = [get_weight(item) for item in train_details]\n",
    "weights = [w for w in weights if w] #removing duplicates or empty "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e657e728",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_weight = sum(weights)/len(weights)\n",
    "print(f\"{average_weight:,.2f}\")\n",
    "\n",
    "#filling the empty weights with average weight \n",
    "def get_weight_with_default(item):\n",
    "    weight = get_weight(item)\n",
    "    return weight or average_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9a0ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#focusing on rank \n",
    "# single product has multiple ranks(in different product categories)\n",
    "# so taking average of all of them \n",
    "def get_rank(item):\n",
    "    rank_dict = item.features.get(\"Best Sellers Rank\")\n",
    "    if rank_dict:\n",
    "        ranks = rank_dict.values()\n",
    "        return sum(ranks)/len(ranks)\n",
    "    return None \n",
    "\n",
    "ranks=[get_rank(item) for item in train_details]\n",
    "ranks = [r for r in ranks if r]\n",
    "average_rank = sum(ranks)/len(ranks)\n",
    "print(f\"{average_rank:,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cd9bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filling empty with average rank\n",
    "def get_rank_with_default(item):\n",
    "    rank = get_rank(item)\n",
    "    return rank or average_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb30813",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_length(item):\n",
    "    return len(item.test_prompt())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e15733f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting a features dictionary \n",
    "def get_features(item):\n",
    "    return {\n",
    "        'weight':get_weight_with_default(item) ,\n",
    "        'rank':get_rank_with_default(item) ,\n",
    "        'is_top_auto_parts_brand' : 1 if is_top_auto_parts_brand(item) else 0 ,\n",
    "        'is_premium_electronics_brand': 1 if is_premium_electronics_brand(item) else 0 ,\n",
    "        'is_top_electronics_brand': 1 if is_top_electronics_brand(item) else 0 ,\n",
    "        'is_top_auto_accessories_brand': 1 if is_top_auto_accessories_brand(item) else 0 ,\n",
    "        'text_length': get_text_length(item) ,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820271f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_features(train_details[35])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d4479b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting to df \n",
    "def list_to_df(items):\n",
    "    features = [get_features(item) for item in items]\n",
    "    df = pd.DataFrame(features)\n",
    "    df['price'] = [item.price for item in items]\n",
    "    return df\n",
    "\n",
    "train_details_df = list_to_df(train_details)\n",
    "test_details_df = list_to_df(test_details)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac17b819",
   "metadata": {},
   "source": [
    "### Baseline Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d075078f",
   "metadata": {},
   "source": [
    "#### Need to have another tester class which tests on the list of the data I have provided "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d23e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tester_baseline:\n",
    "\n",
    "    def __init__(self, predictor, title=None, data=test_details, size=250):\n",
    "        self.predictor = predictor\n",
    "        self.data = data\n",
    "        self.title = title or predictor.__name__.replace(\"_\", \" \").title()\n",
    "        self.size = size\n",
    "        self.guesses = []\n",
    "        self.truths = []\n",
    "        self.errors = []\n",
    "        self.sles = []\n",
    "        self.colors = []\n",
    "\n",
    "    def color_for(self, error, truth):\n",
    "        if error<40 or error/truth < 0.2:\n",
    "            return \"green\"\n",
    "        elif error<80 or error/truth < 0.4:\n",
    "            return \"orange\"\n",
    "        else:\n",
    "            return \"red\"\n",
    "    \n",
    "    def run_datapoint(self, i):\n",
    "        datapoint = self.data[i]\n",
    "        guess = self.predictor(datapoint)\n",
    "        truth = datapoint.price\n",
    "        error = abs(guess - truth)\n",
    "        log_error = math.log(truth+1) - math.log(guess+1)\n",
    "        sle = log_error ** 2\n",
    "        color = self.color_for(error, truth)\n",
    "        title = datapoint.title if len(datapoint.title) <= 40 else datapoint.title[:40]+\"...\"\n",
    "        self.guesses.append(guess)\n",
    "        self.truths.append(truth)\n",
    "        self.errors.append(error)\n",
    "        self.sles.append(sle)\n",
    "        self.colors.append(color)\n",
    "        print(f\"{COLOR_MAP[color]}{i+1}: Guess: ${guess:,.2f} Truth: ${truth:,.2f} Error: ${error:,.2f} SLE: {sle:,.2f} Item: {title}{RESET}\")\n",
    "\n",
    "    def chart(self, title):\n",
    "        max_error = max(self.errors)\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        max_val = max(max(self.truths), max(self.guesses))\n",
    "        plt.plot([0, max_val], [0, max_val], color='deepskyblue', lw=2, alpha=0.6)\n",
    "        plt.scatter(self.truths, self.guesses, s=3, c=self.colors)\n",
    "        plt.xlabel('Ground Truth')\n",
    "        plt.ylabel('Model Estimate')\n",
    "        plt.xlim(0, max_val)\n",
    "        plt.ylim(0, max_val)\n",
    "        plt.title(title)\n",
    "        plt.show()\n",
    "\n",
    "    def report(self):\n",
    "        average_error = sum(self.errors) / self.size\n",
    "        rmsle = math.sqrt(sum(self.sles) / self.size)\n",
    "        hits = sum(1 for color in self.colors if color==\"green\")\n",
    "        title = f\"{self.title} Error=${average_error:,.2f} RMSLE={rmsle:,.2f} Hits={hits/self.size*100:.1f}%\"\n",
    "        self.chart(title)\n",
    "\n",
    "    def run(self):\n",
    "        self.error = 0\n",
    "        for i in range(self.size):\n",
    "            self.run_datapoint(i)\n",
    "        self.report()\n",
    "\n",
    "    @classmethod\n",
    "    def test(cls, function):\n",
    "        cls(function).run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb8e72d",
   "metadata": {},
   "source": [
    "#### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35787eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "train_details_df.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67730737",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = ['weight', 'rank', 'is_top_auto_parts_brand',\n",
    "       'is_premium_electronics_brand', 'is_top_electronics_brand',\n",
    "       'is_top_auto_accessories_brand', 'text_length']\n",
    "\n",
    "X_train = train_details_df[feature_columns]\n",
    "Y_train = train_details_df['price']\n",
    "x_test =test_details_df[feature_columns]\n",
    "y_test =test_details_df['price']\n",
    "\n",
    "model= LinearRegression()\n",
    "model.fit(X_train , Y_train)\n",
    "\n",
    "for feature , coef in zip(feature_columns , model.coef_):\n",
    "    print(f\"{feature}:{coef}\")\n",
    "print(f\"Intercept :{model.intercept_}\")\n",
    "\n",
    "y_pred = model.predict(x_test)\n",
    "mse = mean_squared_error(y_test , y_pred)\n",
    "r2=r2_score(y_test , y_pred)\n",
    "\n",
    "print(f\"RMSE :{(mse)**0.5}\")\n",
    "print(f\"R-squared Score :{r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f96efd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression_pricer(item):\n",
    "    features = get_features(item)\n",
    "    features_df = pd.DataFrame([features])\n",
    "    return model.predict(features_df)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69143019",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tester_baseline.test(linear_regression_pricer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d1ea09",
   "metadata": {},
   "source": [
    "#### BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476ee338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Data Preparation\n",
    "documents = train['test_prompt'].to_list()\n",
    "# test prompt so the model doesn't see the price\n",
    "np.random.seed(42)\n",
    "prices = train['price'].astype(float).to_numpy()\n",
    "# using np array maybe much better for setting random seeds \n",
    "# also LR converts it into array so better for efficiency\n",
    "\n",
    "# 2. Processing and Training\n",
    "vectorizer = CountVectorizer(max_features=1000, stop_words='english')\n",
    "X = vectorizer.fit_transform(documents)\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X, prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2580f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bow_lr_pricer(item):\n",
    "    x = vectorizer.transform([item.test_prompt])\n",
    "    return max(regressor.predict(x)[0],0)  #ensuring no negetive price is returned , also regressor.predict() return a numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02beef80",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tester.test(bow_lr_pricer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f93b67",
   "metadata": {},
   "source": [
    "#### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a720a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "processed_docs = [simple_preprocess(doc) for doc in documents]\n",
    "\n",
    "#training \n",
    "w2v_mode = Word2Vec(sentences=processed_docs, \n",
    "                        vector_size=400 , \n",
    "                        window=5,\n",
    "                        min_count=2, \n",
    "                        workers=8,\n",
    "                        epochs = 2 ,\n",
    "                    )\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedca748",
   "metadata": {},
   "outputs": [],
   "source": [
    "#taking average for each description of the product (not the best way-good for trial)\n",
    "def document_vector(doc):\n",
    "    doc_words = simple_preprocess(doc)\n",
    "    word_vectors = [w2v_mode.wv[word] for word in doc_words if word in w2v_mode.wv]\n",
    "    return np.mean(word_vectors , axis = 0) if word_vectors else np.zeros(w2v_mode.vector_size)\n",
    "\n",
    "X_w2v = np.array([document_vector(doc) for doc in documents])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880daa3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_lr_regressor = LinearRegression()\n",
    "w2v_lr_regressor.fit(X_w2v,prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da455130",
   "metadata": {},
   "outputs": [],
   "source": [
    "def w2v_lr_pricer(item):\n",
    "    doc = item.test_prompt \n",
    "    doc_vector = document_vector(doc)\n",
    "    return max(0,w2v_lr_regressor.predict([doc_vector])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a31b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tester.test(w2v_lr_pricer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb7514d",
   "metadata": {},
   "source": [
    "#### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578f4824",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45547e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "svr_regressor = LinearSVR()\n",
    "svr_regressor.fit(X_w2v, prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eff78a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def svr_pricer(item):\n",
    "    np.random.seed(42)\n",
    "    doc = item.test_prompt\n",
    "    doc_vector = document_vector(doc)\n",
    "    return max(0,float(svr_regressor.predict([doc_vector])[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a38055",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tester.test(svr_pricer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979b9f39",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797b3bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552a8c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42 , n_jobs=8)\n",
    "rf_model.fit(X_w2v , prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19370bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest_pricer(item):\n",
    "    doc = item.test_prompt\n",
    "    doc_vector = document_vector(doc)\n",
    "    return max(0,rf_model.predict([doc_vector])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd488302",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tester.test(random_forest_pricer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf377622",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
