{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Setup and Configuration"
      ],
      "metadata": {
        "id": "OAkXe9AN-9zZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_NlK25YX-tKq"
      },
      "outputs": [],
      "source": [
        "# pip installs\n",
        "\n",
        "%pip install -q --upgrade torch==2.5.1+cu124 torchvision==0.20.1+cu124 torchaudio==2.5.1+cu124 --index-url https://download.pytorch.org/whl/cu124\n",
        "%pip install -q --upgrade requests==2.32.4 bitsandbytes==0.46.0 transformers==4.48.3 accelerate==1.3.0 peft==0.14.0 trl==0.14.0 fsspec==2025.3.0 matplotlib wandb datasets\n",
        "\n",
        "print(\"Installation successfull.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# imports\n",
        "\n",
        "import os\n",
        "import re\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import transformers\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, set_seed\n",
        "from datasets import load_dataset, Dataset, DatasetDict\n",
        "from datetime import datetime\n",
        "from peft import PeftModel\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "UUZfa4ld_WWY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#constants\n",
        "QUANT_4BIT = True\n",
        "BASE_MODEL = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
        "DATASET_NAME = \"Vishy08/pricer-data\""
      ],
      "metadata": {
        "id": "v2e2Z-IkIkli"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hf_token = userdata.get(\"HF_TOKEN\")\n",
        "\n",
        "login(hf_token,add_to_git_credential = True )"
      ],
      "metadata": {
        "id": "EzoCMMTbDrWZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading the model and data"
      ],
      "metadata": {
        "id": "psEyJSs8MGJJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#loading the data\n",
        "dataset = load_dataset(DATASET_NAME)\n",
        "train = dataset['train']\n",
        "test = dataset['test']\n",
        "val = dataset['val']"
      ],
      "metadata": {
        "id": "sKu9GkQXG8yI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if QUANT_4BIT:\n",
        "  quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        "  )\n",
        "else:\n",
        "  quant_config = BitsAndBytesConfig(\n",
        "    load_in_8bit=True,\n",
        "    bnb_8bit_compute_dtype=torch.bfloat16\n",
        "  )"
      ],
      "metadata": {
        "id": "OlOz20FzJAl_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True,device_map=\"auto\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\""
      ],
      "metadata": {
        "collapsed": true,
        "id": "4aoDQzjZDSdt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#loading quantized base model\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL ,\n",
        "    quantization_config = quant_config ,\n",
        "    device_map =\"auto\"\n",
        ")\n",
        "\n",
        "base_model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "fine_tuned_model = PeftModel.from_pretrained(base_model, \"Vishy08/product-pricer-08-12-2025_04.35.08\")\n",
        "\n",
        "print(f\"Memory footprint: {fine_tuned_model.get_memory_footprint() / 1e6:.1f} MB\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "CK14kF0_JPPb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fine_tuned_model\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "S5fwnsxfJo4S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inference and Testing"
      ],
      "metadata": {
        "id": "y_t4i8fkMCHj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_pricer(s):\n",
        "    if \"Price is $\" in s:\n",
        "        # Isolating the part AFTER the label\n",
        "        text_after = s.split(\"Price is $\")[1]\n",
        "        clean_text = text_after.replace(',', '')\n",
        "        match = re.search(r\"[-+]?(?:\\d+\\.?\\d*|\\.\\d+)\", clean_text)\n",
        "        if match:\n",
        "            try:\n",
        "                return float(match.group())\n",
        "            except ValueError:\n",
        "                return 0.0\n",
        "\n",
        "    return 0.0"
      ],
      "metadata": {
        "id": "aKxMsNw6Lxuf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extract_pricer(\"Price is $799.9. fuck you bitch\")"
      ],
      "metadata": {
        "id": "mdHVbe6mMpmI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_predict(datapoint):\n",
        "    set_seed(42)\n",
        "    prompt = datapoint['test_prompt']\n",
        "    inputs = tokenizer.encode(prompt , return_tensors=\"pt\").to(\"cuda\")\n",
        "    attention_mask = torch.ones(inputs.shape , device = 'cuda')\n",
        "    outputs = fine_tuned_model.generate(inputs ,\n",
        "                                        attention_mask = attention_mask ,\n",
        "                                        max_new_tokens = 3 ,\n",
        "                                        num_return_sequences = 1)\n",
        "\n",
        "\n",
        "    response = tokenizer.decode(outputs[0])\n",
        "    return extract_pricer(response)"
      ],
      "metadata": {
        "id": "4t10HxpeMu7V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Tester Class"
      ],
      "metadata": {
        "id": "Bgly3b6PNtpx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "# COLOR MAP for terminal output\n",
        "GREEN = \"\\033[92m\"\n",
        "ORANGE = \"\\033[93m\"\n",
        "RED = \"\\033[91m\"\n",
        "RESET = \"\\033[0m\"\n",
        "COLOR_MAP = {\"red\": RED, \"orange\": ORANGE, \"green\": GREEN}\n",
        "\n",
        "class Tester:\n",
        "    def __init__(self, predictor, data, title=None, size=250):\n",
        "        self.predictor = predictor\n",
        "        self.data = data\n",
        "        self.title = title or predictor.__name__.replace(\"_\", \" \").title()\n",
        "        self.size = size\n",
        "\n",
        "        # Use numpy arrays for numerical data\n",
        "        self.guesses = np.zeros(size)\n",
        "        self.truths = np.zeros(size)\n",
        "        self.errors = np.zeros(size)\n",
        "        self.lche = np.zeros(size)\n",
        "        self.sles = np.zeros(size)\n",
        "        self.colors = []  # Keep as list for strings\n",
        "\n",
        "        # Counters\n",
        "        self.green_count = 0\n",
        "        self.orange_count = 0\n",
        "        self.red_count = 0\n",
        "\n",
        "        # Cache for computed metrics\n",
        "        self._average_error = None\n",
        "        self._rmsle = None\n",
        "\n",
        "    def run_datapoint(self, i):\n",
        "        try:\n",
        "            datapoint = self.data[i] # Changed .iloc[i] to [i] for datasets.Dataset objects\n",
        "            guess = float(self.predictor(datapoint)) # Pass the entire datapoint object\n",
        "            truth = float(datapoint['price'])\n",
        "\n",
        "            error = abs(truth - guess)\n",
        "            log_error = math.log(truth + 1) - math.log(guess + 1)\n",
        "            sle = log_error ** 2\n",
        "            log_cosh_error = self.safe_log_cosh(error)\n",
        "\n",
        "            color = self.color_for(error, truth)\n",
        "\n",
        "            # Update counters\n",
        "            if color == 'green':\n",
        "                self.green_count += 1\n",
        "            elif color == 'orange':\n",
        "                self.orange_count += 1\n",
        "            else:  # red\n",
        "                self.red_count += 1\n",
        "\n",
        "            # Store results in arrays\n",
        "            self.guesses[i] = guess\n",
        "            self.truths[i] = truth\n",
        "            self.errors[i] = error\n",
        "            self.lche[i] = log_cosh_error\n",
        "            self.sles[i] = sle\n",
        "            self.colors.append(color)\n",
        "\n",
        "            return color\n",
        "\n",
        "        except (ValueError, TypeError, AttributeError) as e:\n",
        "            print(f\"Error processing datapoint {i}: {e}\")\n",
        "            self.colors.append('red')\n",
        "            self.red_count += 1\n",
        "            return 'red'\n",
        "\n",
        "    def safe_log_cosh(self, x):\n",
        "        \"\"\"Avoids overflow in log cosh calculation\"\"\"\n",
        "        x = max(min(x, 500), -500)  # Cap between -500 and 500\n",
        "        return math.log(math.cosh(x))\n",
        "\n",
        "    def color_for(self, error, truth):\n",
        "        if error < 40 or error/truth < 0.2:\n",
        "            return 'green'\n",
        "        elif error < 80 or error/truth < 0.4:\n",
        "            return 'orange'\n",
        "        else:\n",
        "            return 'red'\n",
        "\n",
        "    @property\n",
        "    def average_error(self):\n",
        "        if self._average_error is None:\n",
        "            self._average_error = np.mean(self.errors)\n",
        "        return self._average_error\n",
        "\n",
        "    @property\n",
        "    def rmsle(self):\n",
        "        if self._rmsle is None:\n",
        "            self._rmsle = math.sqrt(np.mean(self.sles))\n",
        "        return self._rmsle\n",
        "\n",
        "    def chart(self, title):\n",
        "        plt.figure(figsize=(12, 8))\n",
        "\n",
        "        max_val = max(np.max(self.truths), np.max(self.guesses))\n",
        "\n",
        "        # Add error bands\n",
        "        x = np.linspace(0, max_val, 100)\n",
        "        plt.fill_between(x, x*0.8, x*1.2, color='green', alpha=0.1, label='±20% Range')\n",
        "        plt.fill_between(x, x*0.6, x*1.4, color='orange', alpha=0.1, label='±40% Range')\n",
        "\n",
        "        # Perfect prediction line\n",
        "        plt.plot([0, max_val], [0, max_val], color='skyblue', lw=2,\n",
        "                alpha=0.6, label='Perfect Prediction')\n",
        "\n",
        "        # Scatter plot\n",
        "        # Use self.colors directly as it already contains valid matplotlib color strings\n",
        "        plt.scatter(self.truths, self.guesses, s=50, c=self.colors,\n",
        "                   alpha=0.6, label='Predictions')\n",
        "\n",
        "        # Statistics text box\n",
        "        green_pct = (self.green_count/self.size*100)\n",
        "        orange_pct = (self.orange_count/self.size*100)\n",
        "        red_pct = (self.red_count/self.size*100)\n",
        "\n",
        "        stats_text = (\n",
        "            f'Accuracy Distribution:\\n'\n",
        "            f'Green: {green_pct:.1f}%\\n'\n",
        "            f'Orange: {orange_pct:.1f}%\\n'\n",
        "            f'Red: {red_pct:.1f}%\\n'\n",
        "            f'Average Error: ${self.average_error:,.2f}\\n'\n",
        "            f'RMSLE: {self.rmsle:.2f}'\n",
        "        )\n",
        "\n",
        "        plt.text(0.02, 0.98, stats_text,\n",
        "                transform=plt.gca().transAxes,\n",
        "                verticalalignment='top',\n",
        "                bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
        "\n",
        "        # Customize plot\n",
        "        plt.xlabel('True Values ($)', fontsize=12)\n",
        "        plt.ylabel('Predicted Values ($)', fontsize=12)\n",
        "        plt.title(title, fontsize=14, pad=20)\n",
        "        plt.xlim(0, max_val)\n",
        "        plt.ylim(0, max_val)\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.legend(loc='lower right')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def report(self):\n",
        "        # Print summary statistics with color\n",
        "        print(\"\\nTest Results Summary:\")\n",
        "        print(f\"Total Predictions: {self.size}\")\n",
        "        print(f\"{GREEN}Correct (Green): {self.green_count} ({self.green_count/self.size*100:.1f}%){RESET}\")\n",
        "        print(f\"{ORANGE}Close (Orange): {self.orange_count} ({self.orange_count/self.size*100:.1f}%){RESET}\")\n",
        "        print(f\"{RED}Wrong (Red): {self.red_count} ({self.red_count/self.size*100:.1f}%){RESET}\")\n",
        "        print(f\"Average Error: ${self.average_error:,.2f}\")\n",
        "        print(f\"RMSLE: {self.rmsle:.2f}\")\n",
        "\n",
        "        title = f\"{self.title} Error=${self.average_error:,.2f}  RMSLE={self.rmsle:.2f}  HIT={self.green_count/self.size*100:.1f}%\"\n",
        "        self.chart(title)\n",
        "\n",
        "    def run(self):\n",
        "        # Progress bar for overall testing\n",
        "        with tqdm(total=self.size, desc=\"Testing Progress\",\n",
        "                 bar_format=\"{desc}: {percentage:3.0f}%|{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]\") as pbar:\n",
        "\n",
        "            # Progress bar for correct predictions\n",
        "            expected_correct = int(self.size * 0.7)  # Set expected correct to 70%\n",
        "            with tqdm(total=expected_correct, desc=\"Correct Predictions\",\n",
        "                     bar_format=\"{desc}: {percentage:3.0f}%|{bar}| {n_fmt}/{total_fmt}\") as correct_pbar:\n",
        "\n",
        "                for i in range(self.size):\n",
        "                    color = self.run_datapoint(i)\n",
        "                    pbar.update(1)\n",
        "\n",
        "                    if color == 'green' and correct_pbar.n < expected_correct:\n",
        "                        correct_pbar.update(1)\n",
        "\n",
        "        self.report()\n",
        "\n",
        "    @classmethod\n",
        "    def test(cls, function, data):\n",
        "        cls(function, data).run()"
      ],
      "metadata": {
        "id": "qxk72_CTNeVZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Testing"
      ],
      "metadata": {
        "id": "S8g1IL2lOYo5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Tester.test(model_predict , test)"
      ],
      "metadata": {
        "id": "chh6QfEBNwUw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}